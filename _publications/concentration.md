---
title: "Concentration of Cumulative Reward in Markov Decision Processes"
collection: publications
permalink: /publication/concentration
excerpt: ''
date: 2024-12-27
venue: 'Preprint on Arxiv'
paperurl: 'https://arxiv.org/pdf/2411.18551'
citation: 'B. Sayedana, P. E. Caines and A. Mahajan, "Concentration of Cumulative Reward in Markov Decision Processes," arXiv preprint, arXiv:2411.18551, 2024 '
---
In this paper, we investigate the concentration properties of cumulative rewards in Markov
Decision Processes (MDPs), focusing on both asymptotic and non-asymptotic settings.
We introduce a unified approach to characterize reward concentration in MDPs, covering
both infinite-horizon settings (i.e., average and discounted reward frameworks) and finitehorizon setting. Our asymptotic results include the law of large numbers, the central limit
theorem, and the law of iterated logarithms, while our non-asymptotic bounds include
Azuma-Hoeffding-type inequalities and a non-asymptotic version of the law of iterated logarithms. Additionally, we explore two key implications of our results. First, we analyze the
sample path behavior of the difference in rewards between any two stationary policies. Second, we show that two alternative definitions of regret for learning policies proposed in the
literature are rate-equivalent. Our proof techniques rely on a novel martingale decomposition of cumulative rewards, properties of the solution to the policy evaluation fixed-point
equation, and both asymptotic and non-asymptotic concentration results for martingale
difference sequences. <a href="https://bornasayedana.github.io/public-files/concentration.pdf" style="color: blue; text-decoration: underline;">(pdf)</a>
